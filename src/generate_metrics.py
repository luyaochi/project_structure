"""
ç”Ÿæˆé©—è­‰æŒ‡æ¨™å ±å‘Š
"""
import argparse
import json
import sys
import os
from pathlib import Path

# è¨­ç½® Windows æ§åˆ¶å°ç·¨ç¢¼æ”¯æ´ UTF-8
if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except AttributeError:
        # Python < 3.7
        import codecs
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

# æ·»åŠ  src ç›®éŒ„åˆ°è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent))

from verification_metrics import VerificationMetrics


def format_percentage(value: float) -> str:
    """æ ¼å¼åŒ–ç™¾åˆ†æ¯”"""
    return f"{value * 100:.2f}%"


def generate_report(metrics: dict, output_file: str = None):
    """ç”ŸæˆæŒ‡æ¨™å ±å‘Š"""
    report = []
    report.append("# å°ˆæ¡ˆçµæ§‹ç”Ÿæˆå°æŠ—æŒ‡æ¨™å ±å‘Š\n")
    report.append("## ğŸ“Š ç¸½é«”æŒ‡æ¨™\n\n")

    # è¨ˆç®—ç¸½é«”åˆ†æ•¸
    overall_score = (
        metrics['structure_coverage']['overall_coverage'] * 0.3 +
        metrics['file_coverage']['coverage_rate'] * 0.2 +
        metrics['directory_coverage']['coverage_rate'] * 0.2 +
        metrics['template_accuracy']['accuracy_rate'] * 0.1 +
        metrics['hierarchy_accuracy']['overall_accuracy'] * 0.1 +
        metrics['annotation_preservation']['preservation_rate'] * 0.05 +
        metrics['module_independence']['independence_rate'] * 0.05
    ) * 100

    report.append(f"**ç¸½é«”è©•åˆ†**: {overall_score:.2f}/100\n\n")
    report.append("---\n\n")

    # 1. çµæ§‹è¦†è“‹ç‡
    report.append("## 1ï¸âƒ£ çµæ§‹è¦†è“‹ç‡æŒ‡æ¨™\n\n")
    sc = metrics['structure_coverage']
    report.append(f"- **é æœŸç›®éŒ„æ•¸**: {sc['expected_directories']}")
    report.append(f"- **å¯¦éš›ç›®éŒ„æ•¸**: {sc['actual_directories']}")
    report.append(f"- **ç›®éŒ„è¦†è“‹ç‡**: {format_percentage(sc['directory_coverage_rate'])}")
    report.append(f"- **é æœŸæ–‡ä»¶æ•¸**: {sc['expected_files']}")
    report.append(f"- **å¯¦éš›æ–‡ä»¶æ•¸**: {sc['actual_files']}")
    report.append(f"- **æ–‡ä»¶è¦†è“‹ç‡**: {format_percentage(sc['file_coverage_rate'])}")
    report.append(f"- **æ•´é«”è¦†è“‹ç‡**: {format_percentage(sc['overall_coverage'])}\n\n")

    # 2. æ–‡ä»¶è¦†è“‹ç‡
    report.append("## 2ï¸âƒ£ æ–‡ä»¶è¦†è“‹ç‡æŒ‡æ¨™\n\n")
    fc = metrics['file_coverage']
    report.append(f"- **é æœŸæ–‡ä»¶æ•¸**: {fc['expected_count']}")
    report.append(f"- **å¯¦éš›æ–‡ä»¶æ•¸**: {fc['actual_count']}")
    report.append(f"- **åŒ¹é…æ–‡ä»¶æ•¸**: {fc['matched_count']}")
    report.append(f"- **æ–‡ä»¶è¦†è“‹ç‡**: {format_percentage(fc['coverage_rate'])}")
    report.append(f"- **æ–‡ä»¶æº–ç¢ºç‡**: {format_percentage(fc['accuracy_rate'])}")
    if fc['missing_files']:
        report.append(f"\n**ç¼ºå¤±æ–‡ä»¶** ({len(fc['missing_files'])} å€‹):")
        for f in fc['missing_files'][:10]:  # åªé¡¯ç¤ºå‰10å€‹
            report.append(f"  - {f}")
        if len(fc['missing_files']) > 10:
            report.append(f"  - ... é‚„æœ‰ {len(fc['missing_files']) - 10} å€‹")
    if fc['extra_files']:
        report.append(f"\n**é¡å¤–æ–‡ä»¶** ({len(fc['extra_files'])} å€‹):")
        for f in fc['extra_files'][:10]:
            report.append(f"  - {f}")
        if len(fc['extra_files']) > 10:
            report.append(f"  - ... é‚„æœ‰ {len(fc['extra_files']) - 10} å€‹")
    report.append("\n")

    # 3. ç›®éŒ„è¦†è“‹ç‡
    report.append("## 3ï¸âƒ£ ç›®éŒ„è¦†è“‹ç‡æŒ‡æ¨™\n\n")
    dc = metrics['directory_coverage']
    report.append(f"- **é æœŸç›®éŒ„æ•¸**: {dc['expected_count']}")
    report.append(f"- **å¯¦éš›ç›®éŒ„æ•¸**: {dc['actual_count']}")
    report.append(f"- **åŒ¹é…ç›®éŒ„æ•¸**: {dc['matched_count']}")
    report.append(f"- **ç›®éŒ„è¦†è“‹ç‡**: {format_percentage(dc['coverage_rate'])}")
    report.append(f"- **ç›®éŒ„æº–ç¢ºç‡**: {format_percentage(dc['accuracy_rate'])}\n\n")

    # 4. æ¨¡æ¿æº–ç¢ºæ€§
    report.append("## 4ï¸âƒ£ æ¨¡æ¿æº–ç¢ºæ€§æŒ‡æ¨™\n\n")
    ta = metrics['template_accuracy']
    report.append(f"- **ç¸½æª¢æŸ¥é …**: {ta['total_checks']}")
    report.append(f"- **é€šéæª¢æŸ¥**: {ta['passed_checks']}")
    report.append(f"- **æº–ç¢ºç‡**: {format_percentage(ta['accuracy_rate'])}\n\n")

    # 5. å±¤ç´šæº–ç¢ºæ€§
    report.append("## 5ï¸âƒ£ å±¤ç´šæº–ç¢ºæ€§æŒ‡æ¨™\n\n")
    ha = metrics['hierarchy_accuracy']
    report.append("### å°ˆæ¡ˆå±¤ç´š\n")
    pl = ha['project_level']
    report.append(f"- **é€šéæª¢æŸ¥**: {pl['passed']}/{pl['total']}")
    report.append(f"- **æº–ç¢ºç‡**: {format_percentage(pl['accuracy'])}\n")

    report.append("### æ¨¡çµ„å±¤ç´š\n")
    ml = ha['module_level']
    report.append(f"- **é€šéæª¢æŸ¥**: {ml['passed']}/{ml['total']}")
    report.append(f"- **æº–ç¢ºç‡**: {format_percentage(ml['accuracy'])}\n")

    report.append("### åŠŸèƒ½å±¤ç´š\n")
    fl = ha['feature_level']
    report.append(f"- **é€šéæª¢æŸ¥**: {fl['passed']}/{fl['total']}")
    report.append(f"- **æº–ç¢ºç‡**: {format_percentage(fl['accuracy'])}\n")

    report.append(f"### æ•´é«”å±¤ç´šæº–ç¢ºç‡\n")
    report.append(f"- **æ•´é«”æº–ç¢ºç‡**: {format_percentage(ha['overall_accuracy'])}\n\n")

    # 6. è¨»è§£ä¿ç•™ç‡
    report.append("## 6ï¸âƒ£ è¨»è§£ä¿ç•™ç‡æŒ‡æ¨™\n\n")
    ap = metrics['annotation_preservation']
    report.append(f"- **é æœŸè¨»è§£æ•¸**: {ap['expected_count']}")
    report.append(f"- **ä¿ç•™è¨»è§£æ•¸**: {ap['preserved_count']}")
    report.append(f"- **ä¿ç•™ç‡**: {format_percentage(ap['preservation_rate'])}\n\n")

    # 7. æ¨¡çµ„ç¨ç«‹æ€§
    report.append("## 7ï¸âƒ£ æ¨¡çµ„ç¨ç«‹æ€§æŒ‡æ¨™\n\n")
    mi = metrics['module_independence']
    report.append(f"- **ç¸½æª¢æŸ¥é …**: {mi['total_checks']}")
    report.append(f"- **é€šéæª¢æŸ¥**: {mi['passed_checks']}")
    report.append(f"- **ç¨ç«‹æ€§ç‡**: {format_percentage(mi['independence_rate'])}\n\n")

    # ç¸½çµ
    report.append("---\n\n")
    report.append("## ğŸ“ˆ æŒ‡æ¨™ç¸½çµ\n\n")
    report.append("| æŒ‡æ¨™é¡åˆ¥ | è©•åˆ† | ç‹€æ…‹ |\n")
    report.append("|---------|------|------|\n")

    indicators = [
        ('çµæ§‹è¦†è“‹ç‡', sc['overall_coverage'] * 100, 'âœ…' if sc['overall_coverage'] >= 0.95 else 'âš ï¸' if sc['overall_coverage'] >= 0.8 else 'âŒ'),
        ('æ–‡ä»¶è¦†è“‹ç‡', fc['coverage_rate'] * 100, 'âœ…' if fc['coverage_rate'] >= 0.95 else 'âš ï¸' if fc['coverage_rate'] >= 0.8 else 'âŒ'),
        ('ç›®éŒ„è¦†è“‹ç‡', dc['coverage_rate'] * 100, 'âœ…' if dc['coverage_rate'] >= 0.95 else 'âš ï¸' if dc['coverage_rate'] >= 0.8 else 'âŒ'),
        ('æ¨¡æ¿æº–ç¢ºæ€§', ta['accuracy_rate'] * 100, 'âœ…' if ta['accuracy_rate'] >= 0.9 else 'âš ï¸' if ta['accuracy_rate'] >= 0.7 else 'âŒ'),
        ('å±¤ç´šæº–ç¢ºæ€§', ha['overall_accuracy'] * 100, 'âœ…' if ha['overall_accuracy'] >= 0.9 else 'âš ï¸' if ha['overall_accuracy'] >= 0.7 else 'âŒ'),
        ('è¨»è§£ä¿ç•™ç‡', ap['preservation_rate'] * 100, 'âœ…' if ap['preservation_rate'] >= 0.8 else 'âš ï¸' if ap['preservation_rate'] >= 0.6 else 'âŒ'),
        ('æ¨¡çµ„ç¨ç«‹æ€§', mi['independence_rate'] * 100, 'âœ…' if mi['independence_rate'] >= 0.9 else 'âš ï¸' if mi['independence_rate'] >= 0.7 else 'âŒ'),
    ]

    for name, score, status in indicators:
        report.append(f"| {name} | {score:.2f}% | {status} |\n")

    report.append(f"\n**ç¸½é«”è©•åˆ†**: {overall_score:.2f}/100\n")

    if overall_score >= 90:
        report.append("\nâœ… **å„ªç§€** - ç”Ÿæˆå™¨è¡¨ç¾å„ªç§€ï¼Œå¯ä»¥æŠ•å…¥ä½¿ç”¨\n")
    elif overall_score >= 80:
        report.append("\nâš ï¸ **è‰¯å¥½** - ç”Ÿæˆå™¨è¡¨ç¾è‰¯å¥½ï¼Œå»ºè­°å„ªåŒ–éƒ¨åˆ†æŒ‡æ¨™\n")
    elif overall_score >= 70:
        report.append("\nâš ï¸ **åŠæ ¼** - ç”Ÿæˆå™¨åŸºæœ¬å¯ç”¨ï¼Œéœ€è¦æ”¹é€²\n")
    else:
        report.append("\nâŒ **ä¸åŠæ ¼** - ç”Ÿæˆå™¨éœ€è¦é‡å¤§æ”¹é€²\n")

    report_text = ''.join(report)

    if output_file:
        Path(output_file).write_text(report_text, encoding='utf-8')
        print(f"[OK] æŒ‡æ¨™å ±å‘Šå·²ç”Ÿæˆ: {output_file}")
    else:
        print(report_text)

    return report_text


def main():
    parser = argparse.ArgumentParser(description="ç”Ÿæˆå°ˆæ¡ˆçµæ§‹é©—è­‰æŒ‡æ¨™")
    parser.add_argument('--structure', type=str, default='structure_example.md', help='çµæ§‹å®šç¾©æ–‡ä»¶')
    parser.add_argument('--generated', type=str, required=True, help='ç”Ÿæˆçš„å°ˆæ¡ˆè·¯å¾‘')
    parser.add_argument('--output', type=str, help='è¼¸å‡ºå ±å‘Šæ–‡ä»¶')
    parser.add_argument('--json', action='store_true', help='è¼¸å‡º JSON æ ¼å¼')

    args = parser.parse_args()

    try:
        metrics_calculator = VerificationMetrics(args.structure, args.generated)
        metrics = metrics_calculator.calculate_all_metrics()

        # è¨ˆç®—ç¸½é«”åˆ†æ•¸
        overall_score = (
            metrics['structure_coverage']['overall_coverage'] * 0.3 +
            metrics['file_coverage']['coverage_rate'] * 0.2 +
            metrics['directory_coverage']['coverage_rate'] * 0.2 +
            metrics['template_accuracy']['accuracy_rate'] * 0.1 +
            metrics['hierarchy_accuracy']['overall_accuracy'] * 0.1 +
            metrics['annotation_preservation']['preservation_rate'] * 0.05 +
            metrics['module_independence']['independence_rate'] * 0.05
        ) * 100
        metrics['overall_score'] = overall_score

        if args.json:
            output = json.dumps(metrics, indent=2, ensure_ascii=False)
            if args.output:
                Path(args.output).write_text(output, encoding='utf-8')
            else:
                print(output)
        else:
            generate_report(metrics, args.output)

    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
