"""
ç”Ÿæˆé©—è­‰æŒ‡æ¨™å ±å‘Š
"""
import argparse
import json
import sys
import os
from pathlib import Path

# è¨­ç½® Windows æ§åˆ¶å°ç·¨ç¢¼æ”¯æ´ UTF-8
if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except AttributeError:
        # Python < 3.7
        import codecs
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

# æ·»åŠ  src ç›®éŒ„åˆ°è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent))

from verification_metrics import VerificationMetrics
from i18n import get_text, get_lang_suffix, LANG_EN, LANG_ZH_CN, LANG_ZH_TW, DEFAULT_LANG


def format_percentage(value: float) -> str:
    """æ ¼å¼åŒ–ç™¾åˆ†æ¯”"""
    return f"{value * 100:.2f}%"


def generate_report(metrics: dict, output_file: str = None, lang: str = DEFAULT_LANG):
    """ç”ŸæˆæŒ‡æ¨™å ±å‘Š"""
    t = lambda key: get_text(key, lang)

    report = []
    report.append(f"# {t('metrics_title')}\n")
    report.append(f"## ğŸ“Š {t('overall_metrics')}\n\n")

    # è¨ˆç®—ç¸½é«”åˆ†æ•¸
    overall_score = (
        metrics['structure_coverage']['overall_coverage'] * 0.3 +
        metrics['file_coverage']['coverage_rate'] * 0.2 +
        metrics['directory_coverage']['coverage_rate'] * 0.2 +
        metrics['template_accuracy']['accuracy_rate'] * 0.1 +
        metrics['hierarchy_accuracy']['overall_accuracy'] * 0.1 +
        metrics['annotation_preservation']['preservation_rate'] * 0.05 +
        metrics['module_independence']['independence_rate'] * 0.05
    ) * 100

    report.append(f"**{t('overall_score')}**: {overall_score:.2f}/100\n\n")
    report.append("---\n\n")

    # 1. çµæ§‹è¦†è“‹ç‡
    report.append(f"## 1ï¸âƒ£ {t('structure_coverage_title')}\n\n")
    sc = metrics['structure_coverage']
    report.append(f"- **{t('expected_directories')}**: {sc['expected_directories']}")
    report.append(f"- **{t('actual_directories')}**: {sc['actual_directories']}")
    report.append(f"- **{t('directory_coverage_rate')}**: {format_percentage(sc['directory_coverage_rate'])}")
    report.append(f"- **{t('expected_files')}**: {sc['expected_files']}")
    report.append(f"- **{t('actual_files')}**: {sc['actual_files']}")
    report.append(f"- **{t('file_coverage_rate')}**: {format_percentage(sc['file_coverage_rate'])}")
    report.append(f"- **{t('overall_coverage')}**: {format_percentage(sc['overall_coverage'])}\n\n")

    # 2. æ–‡ä»¶è¦†è“‹ç‡
    report.append(f"## 2ï¸âƒ£ {t('file_coverage_title')}\n\n")
    fc = metrics['file_coverage']
    report.append(f"- **{t('expected_files')}**: {fc['expected_count']}")
    report.append(f"- **{t('actual_files')}**: {fc['actual_count']}")
    report.append(f"- **{t('matched_count')}**: {fc['matched_count']}")
    report.append(f"- **{t('file_coverage_rate')}**: {format_percentage(fc['coverage_rate'])}")
    report.append(f"- **{t('accuracy_rate')}**: {format_percentage(fc['accuracy_rate'])}")
    if fc['missing_files']:
        report.append(f"\n**{t('missing_files')}** ({len(fc['missing_files'])} {t('items')}):")
        for f in fc['missing_files'][:10]:  # åªé¡¯ç¤ºå‰10å€‹
            report.append(f"  - {f}")
        if len(fc['missing_files']) > 10:
            report.append(f"  - ... {t('more')} {len(fc['missing_files']) - 10} {t('items')}")
    if fc['extra_files']:
        report.append(f"\n**{t('extra_files')}** ({len(fc['extra_files'])} {t('items')}):")
        for f in fc['extra_files'][:10]:
            report.append(f"  - {f}")
        if len(fc['extra_files']) > 10:
            report.append(f"  - ... {t('more')} {len(fc['extra_files']) - 10} {t('items')}")
    report.append("\n")

    # 3. ç›®éŒ„è¦†è“‹ç‡
    report.append(f"## 3ï¸âƒ£ {t('directory_coverage_title')}\n\n")
    dc = metrics['directory_coverage']
    report.append(f"- **{t('expected_directories')}**: {dc['expected_count']}")
    report.append(f"- **{t('actual_directories')}**: {dc['actual_count']}")
    report.append(f"- **{t('matched_count')}**: {dc['matched_count']}")
    report.append(f"- **{t('directory_coverage_rate')}**: {format_percentage(dc['coverage_rate'])}")
    report.append(f"- **{t('accuracy_rate')}**: {format_percentage(dc['accuracy_rate'])}\n\n")

    # 4. æ¨¡æ¿æº–ç¢ºæ€§
    report.append(f"## 4ï¸âƒ£ {t('template_accuracy_title')}\n\n")
    ta = metrics['template_accuracy']
    report.append(f"- **{t('total_checks')}**: {ta['total_checks']}")
    report.append(f"- **{t('passed_checks')}**: {ta['passed_checks']}")
    report.append(f"- **{t('accuracy_rate')}**: {format_percentage(ta['accuracy_rate'])}\n\n")

    # 5. å±¤ç´šæº–ç¢ºæ€§
    report.append(f"## 5ï¸âƒ£ {t('hierarchy_accuracy_title')}\n\n")
    ha = metrics['hierarchy_accuracy']
    report.append(f"### {t('project_level')}\n")
    pl = ha['project_level']
    report.append(f"- **{t('passed_checks')}**: {pl['passed']}/{pl['total']}")
    report.append(f"- **{t('accuracy_rate')}**: {format_percentage(pl['accuracy'])}\n")

    report.append(f"### {t('module_level')}\n")
    ml = ha['module_level']
    report.append(f"- **{t('passed_checks')}**: {ml['passed']}/{ml['total']}")
    report.append(f"- **{t('accuracy_rate')}**: {format_percentage(ml['accuracy'])}\n")

    report.append(f"### {t('feature_level')}\n")
    fl = ha['feature_level']
    report.append(f"- **{t('passed_checks')}**: {fl['passed']}/{fl['total']}")
    report.append(f"- **{t('accuracy_rate')}**: {format_percentage(fl['accuracy'])}\n")

    report.append(f"### {t('overall_accuracy')}\n")
    report.append(f"- **{t('overall_accuracy')}**: {format_percentage(ha['overall_accuracy'])}\n\n")

    # 6. è¨»è§£ä¿ç•™ç‡
    report.append(f"## 6ï¸âƒ£ {t('annotation_preservation_title')}\n\n")
    ap = metrics['annotation_preservation']
    report.append(f"- **{t('expected_annotations')}**: {ap['expected_count']}")
    report.append(f"- **{t('preserved_annotations')}**: {ap['preserved_count']}")
    report.append(f"- **{t('preservation_rate')}**: {format_percentage(ap['preservation_rate'])}\n\n")

    # 7. æ¨¡çµ„ç¨ç«‹æ€§
    report.append(f"## 7ï¸âƒ£ {t('module_independence_title')}\n\n")
    mi = metrics['module_independence']
    report.append(f"- **{t('total_checks')}**: {mi['total_checks']}")
    report.append(f"- **{t('passed_checks')}**: {mi['passed_checks']}")
    report.append(f"- **{t('independence_rate')}**: {format_percentage(mi['independence_rate'])}\n\n")

    # ç¸½çµ
    report.append("---\n\n")
    report.append(f"## ğŸ“ˆ {t('metrics_summary')}\n\n")
    report.append(f"| {t('metric_category')} | {t('score')} | {t('status')} |\n")
    report.append("|---------|------|------|\n")

    indicators = [
        (t('structure_coverage'), sc['overall_coverage'] * 100, 'âœ…' if sc['overall_coverage'] >= 0.95 else 'âš ï¸' if sc['overall_coverage'] >= 0.8 else 'âŒ'),
        (t('file_coverage'), fc['coverage_rate'] * 100, 'âœ…' if fc['coverage_rate'] >= 0.95 else 'âš ï¸' if fc['coverage_rate'] >= 0.8 else 'âŒ'),
        (t('directory_coverage'), dc['coverage_rate'] * 100, 'âœ…' if dc['coverage_rate'] >= 0.95 else 'âš ï¸' if dc['coverage_rate'] >= 0.8 else 'âŒ'),
        (t('template_accuracy'), ta['accuracy_rate'] * 100, 'âœ…' if ta['accuracy_rate'] >= 0.9 else 'âš ï¸' if ta['accuracy_rate'] >= 0.7 else 'âŒ'),
        (t('hierarchy_accuracy'), ha['overall_accuracy'] * 100, 'âœ…' if ha['overall_accuracy'] >= 0.9 else 'âš ï¸' if ha['overall_accuracy'] >= 0.7 else 'âŒ'),
        (t('annotation_preservation'), ap['preservation_rate'] * 100, 'âœ…' if ap['preservation_rate'] >= 0.8 else 'âš ï¸' if ap['preservation_rate'] >= 0.6 else 'âŒ'),
        (t('module_independence'), mi['independence_rate'] * 100, 'âœ…' if mi['independence_rate'] >= 0.9 else 'âš ï¸' if mi['independence_rate'] >= 0.7 else 'âŒ'),
    ]

    for name, score, status in indicators:
        report.append(f"| {name} | {score:.2f}% | {status} |\n")

    report.append(f"\n**{t('overall_score')}**: {overall_score:.2f}/100\n")

    if overall_score >= 90:
        report.append(f"\nâœ… **{t('excellent')}** - {t('excellent_desc')}\n")
    elif overall_score >= 80:
        report.append(f"\nâš ï¸ **{t('good')}** - {t('good_desc')}\n")
    elif overall_score >= 70:
        report.append(f"\nâš ï¸ **{t('pass')}** - {t('pass_desc')}\n")
    else:
        report.append(f"\nâŒ **{t('fail')}** - {t('fail_desc')}\n")

    report_text = ''.join(report)

    if output_file:
        Path(output_file).write_text(report_text, encoding='utf-8')
        print(f"[OK] {t('report_generated')}: {output_file}")
    else:
        print(report_text)

    return report_text


def main():
    parser = argparse.ArgumentParser(description="ç”Ÿæˆå°ˆæ¡ˆçµæ§‹é©—è­‰æŒ‡æ¨™")
    parser.add_argument('--structure', type=str, default='structure_example.md', help='çµæ§‹å®šç¾©æ–‡ä»¶')
    parser.add_argument('--generated', type=str, required=True, help='ç”Ÿæˆçš„å°ˆæ¡ˆè·¯å¾‘')
    parser.add_argument('--output', type=str, help='è¼¸å‡ºå ±å‘Šæ–‡ä»¶ï¼ˆä¸å«èªè¨€å¾Œç¶´ï¼‰')
    parser.add_argument('--json', action='store_true', help='è¼¸å‡º JSON æ ¼å¼')
    parser.add_argument('--lang', type=str, default=DEFAULT_LANG,
                       choices=[LANG_EN, LANG_ZH_CN, LANG_ZH_TW],
                       help='èªè¨€é¸æ“‡: en, zh-CN, zh-TW (é è¨­: zh-TW)')
    parser.add_argument('--all-langs', action='store_true',
                       help='ç”Ÿæˆæ‰€æœ‰èªè¨€ç‰ˆæœ¬çš„å ±å‘Š')

    args = parser.parse_args()

    try:
        metrics_calculator = VerificationMetrics(args.structure, args.generated)
        metrics = metrics_calculator.calculate_all_metrics()

        # è¨ˆç®—ç¸½é«”åˆ†æ•¸
        overall_score = (
            metrics['structure_coverage']['overall_coverage'] * 0.3 +
            metrics['file_coverage']['coverage_rate'] * 0.2 +
            metrics['directory_coverage']['coverage_rate'] * 0.2 +
            metrics['template_accuracy']['accuracy_rate'] * 0.1 +
            metrics['hierarchy_accuracy']['overall_accuracy'] * 0.1 +
            metrics['annotation_preservation']['preservation_rate'] * 0.05 +
            metrics['module_independence']['independence_rate'] * 0.05
        ) * 100
        metrics['overall_score'] = overall_score

        if args.json:
            output = json.dumps(metrics, indent=2, ensure_ascii=False)
            if args.output:
                Path(args.output).write_text(output, encoding='utf-8')
            else:
                print(output)
        else:
            if args.all_langs:
                # ç”Ÿæˆæ‰€æœ‰èªè¨€ç‰ˆæœ¬
                languages = [LANG_ZH_TW, LANG_ZH_CN, LANG_EN]
                for lang in languages:
                    if args.output:
                        output_file = args.output.replace('.md', '') + get_lang_suffix(lang) + '.md'
                    else:
                        output_file = f"METRICS{get_lang_suffix(lang)}.md"
                    generate_report(metrics, output_file, lang)
            else:
                # ç”Ÿæˆå–®ä¸€èªè¨€ç‰ˆæœ¬
                if args.output:
                    output_file = args.output.replace('.md', '') + get_lang_suffix(args.lang) + '.md'
                else:
                    output_file = f"METRICS{get_lang_suffix(args.lang)}.md"
                generate_report(metrics, output_file, args.lang)

    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
